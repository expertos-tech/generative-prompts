# Prompt Aplicado

Chat GPT Compartilhado > **\[DicionÃ¡rio Termos IA Generativa]**

# Prompt

## Copie e cole o texto abaixo em uma IA Generativa

Quero um guia **prÃ¡tico e comparativo** sobre **DicionÃ¡rio de Termos de IA Generativa**. Cubra os tÃ³picos a seguir para **cada termo** da lista:

* LLM (Large Language Model): Modelo de linguagem de larga escala, treinado em bilhÃµes de tokens, capaz de gerar e compreender texto.â€‹
* Prompt: Entrada de texto usada para guiar o modelo.â€‹
* Context Window (Janela de Contexto): Quantidade mÃ¡xima de tokens que o modelo consegue ler em uma chamada.â€‹
* Token: Unidade mÃ­nima de texto processada pelo modelo (â‰ˆ pedaÃ§o de palavra).â€‹
* Chunk / Chunking: DivisÃ£o de documentos longos em partes menores para indexaÃ§Ã£o em RAG.
* Embedding: RepresentaÃ§Ã£o numÃ©rica (vetor) de um texto, usada para calcular similaridade semÃ¢ntica.â€‹
* Vector DB (Banco Vetorial): Banco de dados otimizado para armazenar embeddings e buscar por proximidade (ex.: Qdrant, Milvus).â€‹
* RAG (Retrieval-Augmented Generation): Pipeline onde o modelo consulta dados externos via embeddings antes de gerar resposta.â€‹
* Reranker: Modelo auxiliar que reordena os resultados recuperados, melhorando a precisÃ£o do RAG.â€‹
* Fine-tuning: Ajuste dos pesos do modelo com novos dados.
* LoRA (Low-Rank Adaptation): TÃ©cnica de fine-tuning leve: treina sÃ³ matrizes extras em vez de todos os pesos.â€‹
* QLoRA: VersÃ£o do LoRA aplicada em modelo quantizado (4 bits), reduzindo custo de treino.â€‹
* QuantizaÃ§Ã£o: CompressÃ£o de pesos do modelo (ex.: float16 â†’ int4) para caber em GPUs menores, com perda mÃ­nima de qualidade.â€‹
* GGUF: Formato otimizado de modelos quantizados para rodar em CPU/GPU via llama.cpp/Ollama.â€‹
* Inference (InferÃªncia): Processo de rodar o modelo para gerar respostas (fase de uso, nÃ£o de treino).
* Batching: Processar vÃ¡rias requisiÃ§Ãµes em paralelo para aumentar throughput em GPU.â€‹
* KV-Cache (Key/Value Cache): MemÃ³ria que guarda estados anteriores da geraÃ§Ã£o, acelerando prompts longos e conversas.â€‹
* Throughput (tok/s): Taxa de tokens gerados por segundo pelo modelo.â€‹
* Latency (TTFT, p95):â€‹
  * TTFT (Time To First Token): tempo atÃ© o primeiro token sair.â€‹
  * p95: latÃªncia que 95% das requisiÃ§Ãµes nÃ£o ultrapassam.

#### DefiniÃ§Ã£o em uma frase
* **Por que importa** (3â€“5 bullets)
* **Como funciona** (use diagramas ASCII simples quando Ãºtil)
* **Exemplo mÃ­nimo** (1â€“2 frases ou pseudo-cÃ³digo)
* **Boas prÃ¡ticas** (5 bullets)
* **Armadilhas comuns** (5 bullets)
* **Custo & performance** (impacto e dicas: batch, quantizaÃ§Ã£o, KV-cache)
* **Checklist â€œpronto para usar/produÃ§Ã£oâ€** (6â€“8 itens marcÃ¡veis)

### Termos a cobrir

LLM (Large Language Model); Prompt; Context Window (Janela de Contexto); Token; Chunk / Chunking; Embedding; Vector DB (Banco Vetorial); RAG (Retrieval-Augmented Generation); Reranker; Fine-tuning; LoRA (Low-Rank Adaptation); QLoRA; QuantizaÃ§Ã£o; GGUF; Inference (InferÃªncia); Batching; KV-Cache (Key/Value Cache); Throughput (tok/s); Latency (TTFT, p95); **Hallucination** (adicione este termo).

#### PÃºblico & contexto

* PÃºblico: **devs, arquitetos de software, estudantes avanÃ§ados**.
* Stack alvo: **open-weights + RAG + modelos quantizados**, com vetores (Qdrant/Milvus) e serving local/cluster.
* Idioma: **PT-BR** com termos tÃ©cnicos em inglÃªs quando canÃ´nicos.
* CenÃ¡rio: estudo rÃ¡pido + consulta durante projeto/palestra.

#### Formato de saÃ­da (siga esta estrutura para **cada** tÃ³pico)

1. **DefiniÃ§Ã£o em uma frase**
2. **Por que importa**
3. **Como funciona**
4. **Passo a passo curto** (5â€“8 passos)
5. **Exemplo mÃ­nimo**
6. **Boas prÃ¡ticas** (5)
7. **Armadilhas comuns** (5)
8. **Custo & performance**
9. **Checklist â€œproduÃ§Ã£oâ€** (6â€“8 itens)

#### Requisitos especÃ­ficos por tema

Use as bases abaixo para manter a terminologia correta e consistente nas definiÃ§Ãµes e exemplos:

* **LLM / Tokens / Context Window:** defina *token* e *janela de contexto* de forma operacional (ex.: 1 token â‰ˆ pedaÃ§o de palavra; limites prÃ¡ticos da janela). ([OpenAI Help Center][1])
* **RAG:** explique que combina **memÃ³ria nÃ£o-paramÃ©trica** (Ã­ndice de vetores) + **gerador**; cite o trabalho clÃ¡ssico. ([arXiv][2])
* **Embedding / Vector DB:** descreva embeddings como vetores semÃ¢nticos e bancos vetoriais como mecanismo de busca por similaridade; traga Qdrant como exemplo. ([qdrant.tech][3])
* **Reranker:** diferencie **bi-encoder** (recuperaÃ§Ã£o) de **cross-encoder** (reranqueamento) e use **bge-reranker-v2** como referÃªncia. ([Hugging Face][4])
* **LoRA / QLoRA / Fine-tuning:** deixe claro que LoRA Ã© **PEFT** (adapters de baixa dimensÃ£o) e QLoRA Ã© fine-tuning **em modelo quantizado 4-bit**. ([Hugging Face][5])
* **QuantizaÃ§Ã£o / GGUF:** explique compressÃ£o de pesos (ex.: fp16 â†’ int4) e cite GGUF no **llama.cpp**. ([Hugging Face][6])
* **Batching / KV-Cache / Throughput / Latency (TTFT, p95):** conecte *batching* e *KV-cache* Ã  melhora de **tok/s** e **TTFT**. ([Hugging Face][7])

---

### Extras no final

Inclua:

* **GlossÃ¡rio resumido** com todas as definiÃ§Ãµes em 1 linha.
* **Cheatsheet** de quando usar: *RAG vs Fine-tuning leve (LoRA/QLoRA) vs nada*.
* **Perguntas frequentes (FAQ)**:

  * Quando RAG falha?
  * Quando LoRA/QLoRA compensa?
  * Como reduzir TTFT e aumentar tok/s (batching, KV-cache)?
  * Como escolher top-k e thresholds de reranker?

> **ObservaÃ§Ã£o:** se algum termo ficar ambÃ­guo, exemplifique com uma chamada mÃ­nima (pseudo-cÃ³digo) e cite a fonte acima usada para a definiÃ§Ã£o.

---

### ðŸ”Ž ReferÃªncias Ãºteis para a IA considerar (para fundamentar o conteÃºdo)

* **RAG (paper original)** â€” *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*. ([arXiv][2])
* **Tokens / contagem** â€” Ajuda OpenAI e *tiktoken cookbook*. ([OpenAI Help Center][1])
* **Context window** â€” IBM / McKinsey explicadores. ([IBM][8])
* **Vector DB / Qdrant** â€” docs e artigos Qdrant. ([qdrant.tech][3])
* **BGE-M3 / Reranker v2** â€” modelos na Hugging Face. ([Hugging Face][4])
* **LoRA (PEFT)** â€” docs HF/PEFT e curso HF. ([Hugging Face][5])
* **QLoRA** â€” paper arXiv / versÃ£o ACM. ([arXiv][9])
* **KV-Cache** â€” docs Transformers. ([Hugging Face][7])
* **GGUF / llama.cpp** â€” docs HF / repositÃ³rio. ([Hugging Face][6])

---

Quer que eu gere tambÃ©m uma **versÃ£o .MD** com este prompt (pronto para subir no seu repositÃ³rio/handout)?

[1]: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them?utm_source=chatgpt.com "What are tokens and how to count them?"
[2]: https://arxiv.org/abs/2005.11401?utm_source=chatgpt.com "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
[3]: https://qdrant.tech/documentation/?utm_source=chatgpt.com "Qdrant Documentation"
[4]: https://huggingface.co/BAAI/bge-m3?utm_source=chatgpt.com "BAAI/bge-m3"
[5]: https://huggingface.co/docs/transformers/en/peft?utm_source=chatgpt.com "PEFT"
[6]: https://huggingface.co/docs/hub/en/gguf-llamacpp?utm_source=chatgpt.com "GGUF usage with llama.cpp"
[7]: https://huggingface.co/docs/transformers/en/cache_explanation?utm_source=chatgpt.com "Caching"
[8]: https://www.ibm.com/think/topics/context-window?utm_source=chatgpt.com "What is a context window?"
[9]: https://arxiv.org/abs/2305.14314?utm_source=chatgpt.com "QLoRA: Efficient Finetuning of Quantized LLMs"
